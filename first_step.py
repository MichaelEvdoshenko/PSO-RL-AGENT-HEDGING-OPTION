import numpy as np
from sup import AgentDQN, HedgingEnv

def main():
    print("üöÄ –ó–ê–ü–£–°–ö –ü–ï–†–í–û–ì–û –û–ë–£–ß–ï–ù–ò–Ø RL –î–õ–Ø –•–ï–î–ñ–ò–†–û–í–ê–ù–ò–Ø")
    print("="*50)
    
    # 1. –°–û–ó–î–ê–ï–ú –°–†–ï–î–£
    print("\n1. –°–æ–∑–¥–∞–µ–º —Å—Ä–µ–¥—É —Ö–µ–¥–∂–∏—Ä–æ–≤–∞–Ω–∏—è...")
    params = [0.04, 2.0, 0.04, 0.3, -0.7]
    env = HedgingEnv(
        S0=150.0,
        T=30/365,  # 30 –¥–Ω–µ–π
        K=155.0,
        q=0.0,
        r=0.02,
        params_option=params
    )
    print(f"   ‚úì –°—Ä–µ–¥–∞ —Å–æ–∑–¥–∞–Ω–∞")
    print(f"   ‚úì –†–∞–∑–º–µ—Ä state: {env.observation_space.shape[0]}")
    print(f"   ‚úì –î–æ—Å—Ç—É–ø–Ω–æ –¥–µ–π—Å—Ç–≤–∏–π: {env.action_space.n}")
    
    # 2. –°–û–ó–î–ê–ï–ú –ê–ì–ï–ù–¢–ê
    print("\n2. –°–æ–∑–¥–∞–µ–º DQN –∞–≥–µ–Ω—Ç–∞...")
    state_dim = env.observation_space.shape[0]
    action_dim = env.action_space.n
    agent = AgentDQN(state_dim=state_dim, action_dim=action_dim)
    print(f"   ‚úì –ê–≥–µ–Ω—Ç —Å–æ–∑–¥–∞–Ω")
    print(f"   ‚úì –ù–∞—á–∞–ª—å–Ω—ã–π epsilon: {agent.epsilon}")
    print(f"   ‚úì Batch size: {agent.batch_size}")
    
    # 3. –ü–†–û–í–ï–†–Ø–ï–ú –°–†–ï–î–£ (1 —ç–ø–∏–∑–æ–¥ —Å–æ —Å–ª—É—á–∞–π–Ω—ã–º–∏ –¥–µ–π—Å—Ç–≤–∏—è–º–∏)
    print("\n3. –¢–µ—Å—Ç–∏—Ä—É–µ–º —Å—Ä–µ–¥—É (1 —ç–ø–∏–∑–æ–¥ —Å–æ —Å–ª—É—á–∞–π–Ω—ã–º–∏ –¥–µ–π—Å—Ç–≤–∏—è–º–∏)...")
    state = env.reset()
    total_reward = 0
    steps = 0
    
    while True:
        # –°–ª—É—á–∞–π–Ω–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ
        action = np.random.randint(0, 3)
        
        # –®–∞–≥ –≤ —Å—Ä–µ–¥–µ
        next_state, reward, done = env.step(action)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–ø—ã—Ç (–¥–ª—è –æ–±—É—á–µ–Ω–∏—è)
        agent.remember(state, action, reward, next_state, done)
        
        # –û–±–Ω–æ–≤–ª—è–µ–º
        state = next_state
        total_reward += reward
        steps += 1
        
        if done:
            break
    
    print(f"   ‚úì –≠–ø–∏–∑–æ–¥ –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {steps} —à–∞–≥–æ–≤")
    print(f"   ‚úì –ò—Ç–æ–≥–æ–≤–∞—è –Ω–∞–≥—Ä–∞–¥–∞: {total_reward:.2f}")
    print(f"   ‚úì –í –ø–∞–º—è—Ç–∏: {len(agent.memory)} –ø—Ä–∏–º–µ—Ä–æ–≤")
    
    # 4. –ü–ï–†–í–û–ï –û–ë–£–ß–ï–ù–ò–ï (–µ—Å–ª–∏ –Ω–∞–∫–æ–ø–∏–ª–∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø—Ä–∏–º–µ—Ä–æ–≤)
    print("\n4. –ü—Ä–æ–±—É–µ–º –æ–±—É—á–∏—Ç—å –∞–≥–µ–Ω—Ç–∞...")
    if len(agent.memory) >= agent.batch_size:
        loss = agent.learn_from_memory()
        print(f"   ‚úì –ü–µ—Ä–≤–æ–µ –æ–±—É—á–µ–Ω–∏–µ –ø—Ä–æ—à–ª–æ!")
        print(f"   ‚úì Loss: {loss:.6f}")
        print(f"   ‚úì –ù–æ–≤—ã–π epsilon: {agent.epsilon:.4f}")
    else:
        print(f"   ‚ö†Ô∏è  –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
        print(f"   ‚ö†Ô∏è  –ù—É–∂–Ω–æ: {agent.batch_size}, –µ—Å—Ç—å: {len(agent.memory)}")
    
    # 5. –û–ë–£–ß–ê–ï–ú –ù–ê –ù–ï–°–ö–û–õ–¨–ö–ò–• –≠–ü–ò–ó–û–î–ê–•
    print("\n5. –û–±—É—á–∞–µ–º –Ω–∞ 10 —ç–ø–∏–∑–æ–¥–∞—Ö...")
    print("-"*40)
    
    for episode in range(1, 11):
        state = env.reset()
        episode_reward = 0
        episode_steps = 0
        done = False
        
        while not done:
            # –ê–≥–µ–Ω—Ç –≤—ã–±–∏—Ä–∞–µ—Ç –¥–µ–π—Å—Ç–≤–∏–µ (—Å exploration!)
            action = agent.act(state)
            
            # –®–∞–≥ –≤ —Å—Ä–µ–¥–µ
            next_state, reward, done = env.step(action)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–ø—ã—Ç
            agent.remember(state, action, reward, next_state, done)
            
            # –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏ –æ–±—É—á–∞–µ–º
            if episode_steps % 4 == 0:  # –û–±—É—á–∞–µ–º –∫–∞–∂–¥—ã–π 4-–π —à–∞–≥
                agent.learn_from_memory()
            
            # –û–±–Ω–æ–≤–ª—è–µ–º
            state = next_state
            episode_reward += reward
            episode_steps += 1
        
        # –í—ã–≤–æ–¥–∏–º –ø—Ä–æ–≥—Ä–µ—Å—Å
        print(f"–≠–ø–∏–∑–æ–¥ {episode:2d}: –ù–∞–≥—Ä–∞–¥–∞ = {episode_reward:7.2f}, "
              f"–®–∞–≥–æ–≤ = {episode_steps:3d}, "
              f"Epsilon = {agent.epsilon:.3f}, "
              f"–ü–∞–º—è—Ç—å = {len(agent.memory)}")
    
    print("-"*40)
    
    # 6. –¢–ï–°–¢–ò–†–£–ï–ú –û–ë–£–ß–ï–ù–ù–û–ì–û –ê–ì–ï–ù–¢–ê
    print("\n6. –¢–µ—Å—Ç–∏—Ä—É–µ–º –æ–±—É—á–µ–Ω–Ω–æ–≥–æ –∞–≥–µ–Ω—Ç–∞ (–±–µ–∑ exploration)...")
    test_rewards = []
    
    for test_ep in range(3):
        state = env.reset()
        test_reward = 0
        done = False
        
        # –í—Ä–µ–º–µ–Ω–Ω–æ –æ—Ç–∫–ª—é—á–∞–µ–º exploration
        old_epsilon = agent.epsilon
        agent.epsilon = 0.0
        
        while not done:
            action = agent.act(state)  # –¢–µ–ø–µ—Ä—å —Ç–æ–ª—å–∫–æ exploitation
            state, reward, done = env.step(action)
            test_reward += reward
        
        # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º epsilon
        agent.epsilon = old_epsilon
        
        test_rewards.append(test_reward)
        print(f"   –¢–µ—Å—Ç {test_ep+1}: –ù–∞–≥—Ä–∞–¥–∞ = {test_reward:.2f}")
    
    # 7. –°–û–•–†–ê–ù–Ø–ï–ú –ú–û–î–ï–õ–¨
    print("\n7. –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å...")
    agent.save("my_first_dqn_agent.pth")
    print("   ‚úì –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ 'my_first_dqn_agent.pth'")
    
    print("\n" + "="*50)
    print("üéâ –í–ê–® –ü–ï–†–í–´–ô RL –ê–ì–ï–ù–¢ –û–ë–£–ß–ï–ù!")
    print("="*50)
    print("\n–ß—Ç–æ –¥–∞–ª—å—à–µ:")
    print("1. –ó–∞–ø—É—Å—Ç–∏—Ç–µ —ç—Ç–æ—Ç —Å–∫—Ä–∏–ø—Ç: python first_training.py")
    print("2. –ü–æ—Å–º–æ—Ç—Ä–∏—Ç–µ –Ω–∞ –Ω–∞–≥—Ä–∞–¥—ã - –æ–Ω–∏ –¥–æ–ª–∂–Ω—ã —É–ª—É—á—à–∞—Ç—å—Å—è")
    print("3. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —É–≤–µ–ª–∏—á–∏—Ç—å —á–∏—Å–ª–æ —ç–ø–∏–∑–æ–¥–æ–≤ –¥–æ 50-100")
    print("4. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ —Å –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏!")

if __name__ == "__main__":
    main()