import numpy as np
import matplotlib.pyplot as plt
import matplotlib.gridspec as gridspec
from RL_AGENT_AND_ENVIRONMENT import HedgingEnv, AgentDQN
from simulate_hekston import compute_price_call_single, calculate_heston_delta
from collections import deque

def train_and_visualize():
    print("üöÄ –û–ë–£–ß–ï–ù–ò–ï –ê–ì–ï–ù–¢–ê –ò –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø –•–ï–î–ñ–ò–†–û–í–ê–ù–ò–Ø (101 –î–ï–ô–°–¢–í–ò–ï)")
    print("="*70)
    
    # ============= 1. –ü–ê–†–ê–ú–ï–¢–†–´ =============
    params = [0.04, 2.0, 0.04, 0.3, -0.7]
    S0 = 150.0
    K = 155.0
    T = 30/365
    r = 0.02
    q = 0.0
    
    # ============= 2. –û–ë–£–ß–ê–ï–ú –ê–ì–ï–ù–¢–ê =============
    print("\nüìö –≠–¢–ê–ü 1: –û–ë–£–ß–ï–ù–ò–ï –ê–ì–ï–ù–¢–ê")
    print("-"*50)
    
    env = HedgingEnv(S0=S0, T=T, K=K, q=q, r=r, params_option=params)
    agent = AgentDQN(state_dim=6, action_dim=101)  # ‚Üê 101 –î–ï–ô–°–¢–í–ò–ï!
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è 101 –¥–µ–π—Å—Ç–≤–∏—è
    agent.epsilon = 0.5          # –ë–æ–ª—å—à–µ exploration
    agent.epsilon_decay = 0.997  # –ú–µ–¥–ª–µ–Ω–Ω–µ–µ decay
    agent.learning_rate = 0.001  # –ú–µ–Ω—å—à–µ learning rate
    agent.batch_size = 100        # –ë–æ–ª—å—à–µ batch
    agent.memory = deque(maxlen=10000)  # –ë–æ–ª—å—à–µ –ø–∞–º—è—Ç–∏
    
    episodes = 1000  # –ú–∏–Ω–∏–º—É–º 500 —ç–ø–∏–∑–æ–¥–æ–≤
    rewards_history = []
    best_reward = -np.inf
    
    for episode in range(1, episodes + 1):
        state = env.reset()
        total_reward = 0
        done = False
        agent.decay_epsilon()
        
        while not done:
            action = agent.act(state)
            next_state, reward, done = env.step(action)
            agent.remember(state, action, reward, next_state, done)
            
            if len(agent.memory) > agent.batch_size:
                agent.learn_from_memory()
            
            state = next_state
            total_reward += reward
        
        rewards_history.append(total_reward)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å
        if total_reward > best_reward:
            best_reward = total_reward
            agent.save("best_agent_101.pth")
        
        if episode % 2 == 0:
            avg_reward = np.mean(rewards_history[-20:]) if len(rewards_history) >= 20 else np.mean(rewards_history)
            print(f"   –≠–ø–∏–∑–æ–¥ {episode:4d}: Reward = {total_reward:8.2f} | "
                  f"Avg = {avg_reward:8.2f} | Epsilon = {agent.epsilon:.3f} | "
                  f"Memory = {len(agent.memory)}")
    
    agent.epsilon = 0.0
    agent.load("best_agent_101.pth")  # –ó–∞–≥—Ä—É–∂–∞–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å!
    print(f"\n‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ! –õ—É—á—à–∞—è –Ω–∞–≥—Ä–∞–¥–∞: {best_reward:.2f}")
    
    # ============= 3. –°–ò–ú–£–õ–Ø–¶–ò–Ø =============
    print("\nüìà –≠–¢–ê–ü 2: –°–ò–ú–£–õ–Ø–¶–ò–Ø –•–ï–î–ñ–ò–†–û–í–ê–ù–ò–Ø")
    print("-"*50)
    
    sim_env = HedgingEnv(S0=S0, T=T, K=K, q=q, r=r, params_option=params)
    state = sim_env.reset()
    
    # –ú–∞—Å—Å–∏–≤—ã –¥–ª—è –¥–∞–Ω–Ω—ã—Ö
    days = []
    stock_prices = []
    option_prices = []
    deltas = []
    hedge_positions = []
    actions_history = []
    actions_percent = []  # ‚Üê –î–ª—è 101 –¥–µ–π—Å—Ç–≤–∏—è!
    portfolio_values = []
    hedge_errors = []
    
    day = 0
    done = False
    
    print("   –°–∏–º—É–ª—è—Ü–∏—è 30 –¥–Ω–µ–π —Ö–µ–¥–∂–∏—Ä–æ–≤–∞–Ω–∏—è —Å 101 –≤–æ–∑–º–æ–∂–Ω—ã–º –¥–µ–π—Å—Ç–≤–∏–µ–º...")
    
    while not done:
        days.append(day)
        stock_prices.append(sim_env.current_price_stock)
        
        option_price = compute_price_call_single(
            sim_env.current_price_stock, K, 
            max(sim_env.rest_of_time, 0.001), 
            r, q, params
        )
        option_prices.append(option_price)
        
        deltas.append(sim_env.current_delta)
        hedge_positions.append(sim_env.current_count_stocks)
        hedge_errors.append(sim_env.hedge_error)
        portfolio_values.append(sim_env.cash + 
                              sim_env.current_price_stock * sim_env.current_count_stocks + 
                              option_price)
        
        # –ê–≥–µ–Ω—Ç –≤—ã–±–∏—Ä–∞–µ—Ç –¥–µ–π—Å—Ç–≤–∏–µ (0-100)
        action = agent.act(state)
        actions_history.append(action)
        actions_percent.append(action)  # 0-100 = –ø—Ä–æ—Ü–µ–Ω—Ç —Ö–µ–¥–∂–∞
        
        state, reward, done = sim_env.step(action)
        day += 1
    
    print(f"‚úÖ –°–∏–º—É–ª—è—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞! –î–Ω–µ–π: {day}")
    
    # ============= 4. –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø =============
    print("\nüé® –≠–¢–ê–ü 3: –°–û–ó–î–ê–ù–ò–ï –ì–†–ê–§–ò–ö–û–í")
    print("-"*50)
    
    fig = plt.figure(figsize=(18, 14))
    gs = gridspec.GridSpec(4, 3, figure=fig, hspace=0.3, wspace=0.3)
    
    # [1] –¶–µ–Ω–∞ –∞–∫—Ü–∏–∏ –∏ –æ–ø—Ü–∏–æ–Ω–∞
    ax1 = fig.add_subplot(gs[0, :])
    ax1.set_title('üìà –¶–µ–Ω–∞ –∞–∫—Ü–∏–∏ –∏ –æ–ø—Ü–∏–æ–Ω–∞', fontsize=14, fontweight='bold', pad=15)
    ax1.plot(days, stock_prices, 'b-', linewidth=2, label='–¶–µ–Ω–∞ –∞–∫—Ü–∏–∏ (S)')
    ax1.set_xlabel('–î–µ–Ω—å')
    ax1.set_ylabel('–¶–µ–Ω–∞ –∞–∫—Ü–∏–∏', color='b')
    ax1.tick_params(axis='y', labelcolor='b')
    ax1.grid(True, alpha=0.3)
    
    ax1b = ax1.twinx()
    ax1b.plot(days, option_prices, 'r-', linewidth=2, label='–¶–µ–Ω–∞ –æ–ø—Ü–∏–æ–Ω–∞')
    ax1b.set_ylabel('–¶–µ–Ω–∞ –æ–ø—Ü–∏–æ–Ω–∞', color='r')
    ax1b.tick_params(axis='y', labelcolor='r')
    
    lines1, labels1 = ax1.get_legend_handles_labels()
    lines1b, labels1b = ax1b.get_legend_handles_labels()
    ax1.legend(lines1 + lines1b, labels1 + labels1b, loc='upper left')
    
    # [2] –î–µ–ª—å—Ç–∞ –∏ –ø–æ–∑–∏—Ü–∏—è —Ö–µ–¥–∂–∞
    ax2 = fig.add_subplot(gs[1, :])
    ax2.set_title('üõ°Ô∏è –î–µ–ª—å—Ç–∞ –æ–ø—Ü–∏–æ–Ω–∞ vs –ü–æ–∑–∏—Ü–∏—è —Ö–µ–¥–∂–∞', fontsize=14, fontweight='bold', pad=15)
    ax2.plot(days, deltas, 'g-', linewidth=2, label='–î–µ–ª—å—Ç–∞ –æ–ø—Ü–∏–æ–Ω–∞')
    ax2.plot(days, hedge_positions, 'orange', linewidth=2, linestyle='--', 
             label='–ü–æ–∑–∏—Ü–∏—è –≤ –∞–∫—Ü–∏—è—Ö')
    ax2.set_xlabel('–î–µ–Ω—å')
    ax2.set_ylabel('–ó–Ω–∞—á–µ–Ω–∏–µ')
    ax2.legend(loc='upper right')
    ax2.grid(True, alpha=0.3)
    ax2.axhline(y=0, color='black', linestyle='-', linewidth=0.5, alpha=0.5)
    
    # [3] –û—à–∏–±–∫–∞ —Ö–µ–¥–∂–∏—Ä–æ–≤–∞–Ω–∏—è
    ax3 = fig.add_subplot(gs[2, 0])
    ax3.set_title('‚ö†Ô∏è –û—à–∏–±–∫–∞ —Ö–µ–¥–∂–∏—Ä–æ–≤–∞–Ω–∏—è', fontsize=12, fontweight='bold')
    ax3.plot(days, hedge_errors, 'purple', linewidth=2, marker='o', markersize=3)
    ax3.set_xlabel('–î–µ–Ω—å')
    ax3.set_ylabel('Hedge Error')
    ax3.axhline(y=0, color='black', linestyle='-', linewidth=0.5)
    ax3.grid(True, alpha=0.3)
    
    # [4] –î–µ–π—Å—Ç–≤–∏—è –∞–≥–µ–Ω—Ç–∞ (101 –¥–µ–π—Å—Ç–≤–∏–µ!)
    ax4 = fig.add_subplot(gs[2, 1])
    ax4.set_title('üéÆ –î–µ–π—Å—Ç–≤–∏—è –∞–≥–µ–Ω—Ç–∞ (% —Ö–µ–¥–∂–∞)', fontsize=12, fontweight='bold')
    
    # –ì—Ä–∞—Ñ–∏–∫ –ø—Ä–æ—Ü–µ–Ω—Ç–∞ —Ö–µ–¥–∂–∞ –ø–æ –¥–Ω—è–º
    days_actions = days[:-1] if len(days) == len(actions_percent) else days[:len(actions_percent)]
    ax4.plot(days_actions, actions_percent, 'b-', linewidth=2, marker='o', markersize=4)
    ax4.fill_between(days_actions, 0, actions_percent, alpha=0.3, color='blue')
    ax4.set_xlabel('–î–µ–Ω—å')
    ax4.set_ylabel('–ü—Ä–æ—Ü–µ–Ω—Ç —Ö–µ–¥–∂–∞ (%)')
    ax4.set_ylim(0, 105)
    ax4.grid(True, alpha=0.3)
    
    # [5] –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–µ–π—Å—Ç–≤–∏–π (–≥–∏—Å—Ç–æ–≥—Ä–∞–º–º–∞)
    ax5 = fig.add_subplot(gs[2, 2])
    ax5.set_title('üìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–µ–π—Å—Ç–≤–∏–π', fontsize=12, fontweight='bold')
    
    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –¥–∏–∞–ø–∞–∑–æ–Ω–∞–º: 0%, 1-25%, 26-50%, 51-75%, 76-99%, 100%
    bins = [0, 1, 25, 50, 75, 99, 100]
    bin_labels = ['0%', '1-25%', '26-50%', '51-75%', '76-99%', '100%']
    
    hist, _ = np.histogram(actions_percent, bins=bins)
    colors = plt.cm.RdYlGn(hist / max(hist) if max(hist) > 0 else hist)
    
    bars = ax5.bar(bin_labels, hist, color=colors, edgecolor='black', alpha=0.7)
    ax5.set_xlabel('–ü—Ä–æ—Ü–µ–Ω—Ç —Ö–µ–¥–∂–∞')
    ax5.set_ylabel('–ß–∞—Å—Ç–æ—Ç–∞')
    ax5.tick_params(axis='x', rotation=45)
    
    # –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –Ω–∞ —Å—Ç–æ–ª–±—Ü—ã
    for bar, count in zip(bars, hist):
        height = bar.get_height()
        ax5.text(bar.get_x() + bar.get_width()/2., height,
                f'{count}', ha='center', va='bottom')
    
    # [6] –°—Ç–æ–∏–º–æ—Å—Ç—å –ø–æ—Ä—Ç—Ñ–µ–ª—è
    ax6 = fig.add_subplot(gs[3, :])
    ax6.set_title('üí∞ –°—Ç–æ–∏–º–æ—Å—Ç—å –ø–æ—Ä—Ç—Ñ–µ–ª—è', fontsize=14, fontweight='bold', pad=15)
    ax6.plot(days, portfolio_values, 'b-', linewidth=2, label='–ü–æ—Ä—Ç—Ñ–µ–ª—å')
    
    # –õ–∏–Ω–µ–π–Ω—ã–π —Ç—Ä–µ–Ω–¥
    if len(days) > 1:
        z = np.polyfit(days, portfolio_values, 1)
        p = np.poly1d(z)
        ax6.plot(days, p(days), 'r--', linewidth=1.5, 
                label=f'–¢—Ä–µ–Ω–¥: {z[0]:.2f} $/–¥–µ–Ω—å')
    
    ax6.set_xlabel('–î–µ–Ω—å')
    ax6.set_ylabel('–°—Ç–æ–∏–º–æ—Å—Ç—å ($)')
    ax6.legend(loc='upper right')
    ax6.grid(True, alpha=0.3)
    
    # –ò—Ç–æ–≥–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    optimal_hedge = np.mean(actions_percent) if actions_percent else 0
    
    fig.text(0.02, 0.02, 
             f"üìä –ò–¢–û–ì–û–í–´–ï –ú–ï–¢–†–ò–ö–ò (101 –¥–µ–π—Å—Ç–≤–∏–µ):\n"
             f"‚Ä¢ –ù–∞—á–∞–ª—å–Ω–∞—è —Ü–µ–Ω–∞ –∞–∫—Ü–∏–∏: ${S0:.1f}\n"
             f"‚Ä¢ –°—Ç—Ä–∞–π–∫: ${K:.1f}\n"
             f"‚Ä¢ –í—Ä–µ–º—è –¥–æ —ç–∫—Å–ø–∏—Ä–∞—Ü–∏–∏: {T*365:.0f} –¥–Ω–µ–π\n"
             f"‚Ä¢ –§–∏–Ω–∞–ª—å–Ω–∞—è —Ü–µ–Ω–∞ –∞–∫—Ü–∏–∏: ${stock_prices[-1]:.2f}\n"
             f"‚Ä¢ –§–∏–Ω–∞–ª—å–Ω–∞—è —Ü–µ–Ω–∞ –æ–ø—Ü–∏–æ–Ω–∞: ${option_prices[-1]:.2f}\n"
             f"‚Ä¢ –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—à–∏–±–∫–∞ —Ö–µ–¥–∂–∞: {hedge_errors[-1]:.4f}\n"
             f"‚Ä¢ –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–æ–∏–º–æ—Å—Ç—å –ø–æ—Ä—Ç—Ñ–µ–ª—è: ${portfolio_values[-1]:.2f}\n"
             f"‚Ä¢ –°—Ä–µ–¥–Ω—è—è –æ—à–∏–±–∫–∞ —Ö–µ–¥–∂–∞: {np.mean(np.abs(hedge_errors)):.4f}\n"
             f"‚Ä¢ –°—Ä–µ–¥–Ω–∏–π –ø—Ä–æ—Ü–µ–Ω—Ç —Ö–µ–¥–∂–∞: {optimal_hedge:.1f}%\n"
             f"‚Ä¢ –õ—É—á—à–∞—è –Ω–∞–≥—Ä–∞–¥–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏: {best_reward:.2f}",
             fontsize=10, bbox=dict(boxstyle="round,pad=0.5", facecolor="lightyellow"))
    
    plt.suptitle('ü§ñ DQN –ê–ì–ï–ù–¢: –î–ò–ù–ê–ú–ò–ß–ï–°–ö–û–ï –•–ï–î–ñ–ò–†–û–í–ê–ù–ò–ï (101 –î–ï–ô–°–¢–í–ò–ï, HESTON)', 
                 fontsize=16, fontweight='bold', y=1.02)
    
    plt.tight_layout()
    plt.savefig('dqn_hedging_101_actions.png', dpi=150, bbox_inches='tight')
    plt.show()
    
    print(f"\n‚úÖ –ì—Ä–∞—Ñ–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ 'dqn_hedging_101_actions.png'")
    print(f"üìä –°—Ä–µ–¥–Ω–∏–π –ø—Ä–æ—Ü–µ–Ω—Ç —Ö–µ–¥–∂–∞: {optimal_hedge:.1f}%")
    print(f"üèÜ –õ—É—á—à–∞—è –Ω–∞–≥—Ä–∞–¥–∞: {best_reward:.2f}")
    
    agent.save("trained_agent_101.pth")
    print("‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ 'trained_agent_101.pth'")
    
    return agent, sim_env

if __name__ == "__main__":
    agent, env = train_and_visualize()
    
    print("\n" + "="*70)
    print("üéØ –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø –ó–ê–í–ï–†–®–ï–ù–ê!")
    print("="*70)
