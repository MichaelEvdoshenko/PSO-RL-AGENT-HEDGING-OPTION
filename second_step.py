# fine_tune_101.py
import numpy as np
import torch
import matplotlib.pyplot as plt
from RL_AGENT_AND_ENVIRONMENT import HedgingEnv, AgentDQN
from simulate_hekston import compute_price_call_single
import os
from collections import deque

def fine_tune_agent():
    print("="*70)
    print("üöÄ FINE-TUNING: –î–û–û–ë–£–ß–ê–ï–ú –ê–ì–ï–ù–¢–ê –ù–ê –°–õ–£–ß–ê–ô–ù–´–• –¢–†–ê–ï–ö–¢–û–†–ò–Ø–•")
    print("="*70)
    
    # ============= 1. –ü–ê–†–ê–ú–ï–¢–†–´ =============
    params = [0.04, 2.0, 0.04, 0.3, -0.7]
    S0 = 150.0
    K = 155.0
    T = 30/365
    r = 0.02
    q = 0.0
    
    # ============= 2. –°–û–ó–î–ê–ï–ú –°–†–ï–î–£ =============
    env = HedgingEnv(
        S0=S0, T=T, K=K, q=q, r=r, 
        params_option=params
    )
    
    # ============= 3. –ó–ê–ì–†–£–ñ–ê–ï–ú –û–ë–£–ß–ï–ù–ù–£–Æ –ú–û–î–ï–õ–¨ =============
    print("\nüìÇ –ó–∞–≥—Ä—É–∑–∫–∞ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏...")
    agent = AgentDQN(state_dim=6, action_dim=101)
    
    try:
        agent.load("best_agent_101.pth")
        print(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞!")
        print(f"   –¢–µ–∫—É—â–∏–π epsilon: {agent.epsilon}")
        print(f"   –†–∞–∑–º–µ—Ä –ø–∞–º—è—Ç–∏: {len(agent.memory)}")
    except:
        print("‚ùå –û—à–∏–±–∫–∞: —Ñ–∞–π–ª best_agent_101.pth –Ω–µ –Ω–∞–π–¥–µ–Ω!")
        return
    
    # ============= 4. –ù–ê–°–¢–†–û–ô–ö–ò –î–õ–Ø FINE-TUNING =============
    # –£–í–ï–õ–ò–ß–ò–í–ê–ï–ú EPSILON –î–õ–Ø –ò–°–°–õ–ï–î–û–í–ê–ù–ò–Ø!
    agent.epsilon = 0.3          # –ù–∞—á–∏–Ω–∞–µ–º —Å 30% —Å–ª—É—á–∞–π–Ω—ã—Ö –¥–µ–π—Å—Ç–≤–∏–π
    agent.epsilon_min = 0.01    # –ú–∏–Ω–∏–º—É–º 1%
    agent.epsilon_decay = 0.997 # –ú–µ–¥–ª–µ–Ω–Ω—ã–π —Å–ø–∞–¥
    
    # –£–ú–ï–ù–¨–®–ê–ï–ú LEARNING RATE –î–õ–Ø –¢–û–ù–ö–û–ô –ù–ê–°–¢–†–û–ô–ö–ò
    agent.learning_rate = 0.0005  # –í 2 —Ä–∞–∑–∞ –º–µ–Ω—å—à–µ!
    agent.optimizer = torch.optim.Adam(
        agent.policy_net.parameters(), 
        lr=agent.learning_rate
    )
    
    # –£–í–ï–õ–ò–ß–ò–í–ê–ï–ú –ü–ê–ú–Ø–¢–¨
    agent.memory = deque(maxlen=10000)
    
    print(f"\n‚öôÔ∏è  –ù–∞—Å—Ç—Ä–æ–π–∫–∏ fine-tuning:")
    print(f"   Epsilon: {agent.epsilon} ‚Üí {agent.epsilon_min}")
    print(f"   Learning rate: {agent.learning_rate}")
    print(f"   Batch size: {agent.batch_size}")
    print(f"   Memory size: {agent.memory.maxlen}")
    
    # ============= 5. –î–û–û–ë–£–ß–ï–ù–ò–ï =============
    print("\nüìö –≠–¢–ê–ü: –î–û–û–ë–£–ß–ï–ù–ò–ï –ù–ê –°–õ–£–ß–ê–ô–ù–´–• –¢–†–ê–ï–ö–¢–û–†–ò–Ø–•")
    print("-"*70)
    
    episodes = 500  # –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª—è fine-tuning!
    best_reward = -np.inf
    rewards_history = []
    hedge_errors_history = []
    
    for episode in range(1, episodes + 1):
        # –ö–ê–ñ–î–´–ô –≠–ü–ò–ó–û–î - –ù–û–í–´–ô –°–õ–£–ß–ê–ô–ù–´–ô SEED!
        # env.reset() —É–∂–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–ª—É—á–∞–π–Ω—ã–π seed
        
        state = env.reset()
        total_reward = 0
        episode_hedge_errors = []
        done = False
        
        while not done:
            # –ê–≥–µ–Ω—Ç –≤—ã–±–∏—Ä–∞–µ—Ç –¥–µ–π—Å—Ç–≤–∏–µ (—Å exploration!)
            action = agent.act(state)
            
            # –®–∞–≥ –≤ —Å—Ä–µ–¥–µ
            next_state, reward, done = env.step(action)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–ø—ã—Ç
            agent.remember(state, action, reward, next_state, done)
            
            # –û–±—É—á–∞–µ–º—Å—è
            if len(agent.memory) > agent.batch_size:
                loss = agent.learn_from_memory()
            
            # –°–æ–±–∏—Ä–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏
            total_reward += reward
            episode_hedge_errors.append(abs(env.hedge_error))
            state = next_state
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Å—Ç–æ—Ä–∏—é
        rewards_history.append(total_reward)
        avg_hedge_error = np.mean(episode_hedge_errors) if episode_hedge_errors else 0
        hedge_errors_history.append(avg_hedge_error)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å
        if total_reward > best_reward:
            best_reward = total_reward
            agent.save("best_agent_101_finetuned.pth")
        
        # –ü—Ä–æ–≥—Ä–µ—Å—Å –∫–∞–∂–¥—ã–µ 20 —ç–ø–∏–∑–æ–¥–æ–≤
        if episode % 20 == 0:
            avg_reward = np.mean(rewards_history[-20:]) if len(rewards_history) >= 20 else np.mean(rewards_history)
            avg_hedge = np.mean(hedge_errors_history[-20:]) if len(hedge_errors_history) >= 20 else np.mean(hedge_errors_history)
            
            print(f"   –≠–ø–∏–∑–æ–¥ {episode:3d}/500 | "
                  f"Reward: {total_reward:8.2f} | "
                  f"Avg Reward: {avg_reward:8.2f} | "
                  f"Hedge Error: {avg_hedge:.4f} | "
                  f"Epsilon: {agent.epsilon:.3f} | "
                  f"Memory: {len(agent.memory)}")
    
    # ============= 6. –°–û–•–†–ê–ù–Ø–ï–ú –§–ò–ù–ê–õ–¨–ù–£–Æ –ú–û–î–ï–õ–¨ =============
    agent.save("agent_101_finetuned_final.pth")
    print(f"\n‚úÖ Fine-tuning –∑–∞–≤–µ—Ä—à–µ–Ω!")
    print(f"   –õ—É—á—à–∞—è –Ω–∞–≥—Ä–∞–¥–∞: {best_reward:.2f}")
    print(f"   –§–∏–Ω–∞–ª—å–Ω—ã–π epsilon: {agent.epsilon:.4f}")
    
    # ============= 7. –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï =============
    print("\nüß™ –≠–¢–ê–ü: –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –î–û–û–ë–£–ß–ï–ù–ù–û–ì–û –ê–ì–ï–ù–¢–ê")
    print("-"*70)
    
    # –û—Ç–∫–ª—é—á–∞–µ–º exploration –¥–ª—è —Ç–µ—Å—Ç–∞
    agent.epsilon = 0.0
    
    test_rewards = []
    test_hedge_errors = []
    
    for test in range(50):
        state = env.reset()
        total_reward = 0
        hedge_errors = []
        done = False
        
        while not done:
            action = agent.act(state)
            state, reward, done = env.step(action)
            total_reward += reward
            hedge_errors.append(abs(env.hedge_error))
        
        test_rewards.append(total_reward)
        test_hedge_errors.append(np.mean(hedge_errors))
    
    print(f"\nüìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø:")
    print(f"   –°—Ä–µ–¥–Ω—è—è –Ω–∞–≥—Ä–∞–¥–∞: {np.mean(test_rewards):.2f} ¬± {np.std(test_rewards):.2f}")
    print(f"   –°—Ä–µ–¥–Ω—è—è –æ—à–∏–±–∫–∞ —Ö–µ–¥–∂–∞: {np.mean(test_hedge_errors):.4f}")
    print(f"   –õ—É—á—à–∞—è –Ω–∞–≥—Ä–∞–¥–∞: {max(test_rewards):.2f}")
    print(f"   –•—É–¥—à–∞—è –Ω–∞–≥—Ä–∞–¥–∞: {min(test_rewards):.2f}")
    
    # ============= 8. –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø –ü–†–û–ì–†–ï–°–°–ê =============
    plt.figure(figsize=(12, 5))
    
    plt.subplot(1, 2, 1)
    plt.plot(rewards_history, alpha=0.6, label='Reward per episode')
    
    # –°–∫–æ–ª—å–∑—è—â–µ–µ —Å—Ä–µ–¥–Ω–µ–µ
    if len(rewards_history) >= 20:
        moving_avg = np.convolve(rewards_history, np.ones(20)/20, mode='valid')
        plt.plot(range(19, len(rewards_history)), moving_avg, 'r-', linewidth=2, label='Moving avg (20)')
    
    plt.xlabel('Episode')
    plt.ylabel('Reward')
    plt.title('Fine-tuning Progress')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.subplot(1, 2, 2)
    plt.plot(hedge_errors_history, alpha=0.6, label='Hedge Error')
    
    if len(hedge_errors_history) >= 20:
        moving_hedge = np.convolve(hedge_errors_history, np.ones(20)/20, mode='valid')
        plt.plot(range(19, len(hedge_errors_history)), moving_hedge, 'r-', linewidth=2, label='Moving avg (20)')
    
    plt.xlabel('Episode')
    plt.ylabel('Hedge Error')
    plt.title('Hedge Error During Fine-tuning')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('fine_tuning_progress.png', dpi=150)
    plt.show()
    
    print(f"\nüìä –ì—Ä–∞—Ñ–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ 'fine_tuning_progress.png'")
    print("="*70)
    print("‚úÖ FINE-TUNING –ó–ê–í–ï–†–®–ï–ù –£–°–ü–ï–®–ù–û!")
    print("="*70)
    
    return agent

def quick_compare():
    """–ë—ã—Å—Ç—Ä–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –¥–æ –∏ –ø–æ—Å–ª–µ fine-tuning"""
    print("\nüîç –ë–´–°–¢–†–û–ï –°–†–ê–í–ù–ï–ù–ò–ï:")
    print("-"*70)
    
    params = [0.04, 2.0, 0.04, 0.3, -0.7]
    env = HedgingEnv(S0=150.0, T=30/365, K=155.0, q=0.0, r=0.02, params_option=params)
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –û–ë–ï –º–æ–¥–µ–ª–∏
    agent_old = AgentDQN(state_dim=6, action_dim=101)
    agent_new = AgentDQN(state_dim=6, action_dim=101)
    
    try:
        agent_old.load("best_agent_101.pth")
        agent_new.load("best_agent_101_finetuned.pth")
    except:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª–∏ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è")
        return
    
    # –û—Ç–∫–ª—é—á–∞–µ–º exploration
    agent_old.epsilon = 0.0
    agent_new.epsilon = 0.0
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º –Ω–∞ 10 —Å–ª—É—á–∞–π–Ω—ã—Ö —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏—è—Ö
    old_rewards = []
    new_rewards = []
    
    for _ in range(10):
        # –û–¥–∏–Ω–∞–∫–æ–≤—ã–π seed –¥–ª—è —á–µ—Å—Ç–Ω–æ–≥–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        seed = np.random.randint(0, 10000)
        
        # –¢–µ—Å—Ç —Å—Ç–∞—Ä–æ–π –º–æ–¥–µ–ª–∏
        np.random.seed(seed)
        state = env.reset()
        old_reward = 0
        done = False
        while not done:
            action = agent_old.act(state)
            state, reward, done = env.step(action)
            old_reward += reward
        old_rewards.append(old_reward)
        
        # –¢–µ—Å—Ç –Ω–æ–≤–æ–π –º–æ–¥–µ–ª–∏
        np.random.seed(seed)
        state = env.reset()
        new_reward = 0
        done = False
        while not done:
            action = agent_new.act(state)
            state, reward, done = env.step(action)
            new_reward += reward
        new_rewards.append(new_reward)
    
    print(f"\nüìä –°–†–ê–í–ù–ï–ù–ò–ï –ù–ê –û–î–ò–ù–ê–ö–û–í–´–• –¢–†–ê–ï–ö–¢–û–†–ò–Ø–•:")
    print(f"   –î–æ fine-tuning:  {np.mean(old_rewards):.2f} ¬± {np.std(old_rewards):.2f}")
    print(f"   –ü–æ—Å–ª–µ fine-tuning: {np.mean(new_rewards):.2f} ¬± {np.std(new_rewards):.2f}")
    print(f"   –£–ª—É—á—à–µ–Ω–∏–µ: {((np.mean(new_rewards) - np.mean(old_rewards)) / abs(np.mean(old_rewards)) * 100):+.1f}%")

if __name__ == "__main__":
    # –ó–ê–ü–£–°–ö FINE-TUNING
    agent = fine_tune_agent()
    
    # –ë–´–°–¢–†–û–ï –°–†–ê–í–ù–ï–ù–ò–ï
    quick_compare()